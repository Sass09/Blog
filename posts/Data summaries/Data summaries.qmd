---
title: "Data Summaries"
author: Shifa Maqsood
---
```{r setup, include=FALSE}
library(tidyverse)   # data wrangling functions

library(kableExtra)  # for nice tables

```

# Social media data and summarise ()
```{r}
tweets <- readRDS("ncod_tweets.rds")
favourite_summary <- summarise(tweets,
                           mean_favs = mean(favorite_count),
                           median_favs = median(favorite_count),
                           min_favs = min(favorite_count),
                           max_favs = max(favorite_count))

```



```{r}
tweet_summary <- tweets %>%
  summarise(mean_favs = mean(favorite_count),
            median_favs = quantile(favorite_count, .5),
            n = n(),
            min_date = min(created_at),
            max_date = max(created_at))

glimpse(tweet_summary)
```





**GGPLOT of favorite_count** 


```{r}
ggplot(tweets, aes(x = favorite_count)) +
  geom_histogram(bins = 25) +
  scale_x_continuous(trans = "pseudo_log", #it goes up in intervals of magnitude and it helps see how much is between each interval 
                     breaks = c(0, 1, 10, 100, 1000, 10000))



```

```{r}
tweets$source[2] #displays the second value in the row
# %>% this is a pipe operator and it can be used to send output from one function into another 
tweet_summary <- tweets %>% # start with the object tweets and then
  summarise(mean_favs = mean(favorite_count), #summarise it
            median_favs = median(favorite_count))

```

```{r}
tweet_summary <- tweets %>%
  summarise(mean_favs = mean(favorite_count),
            median_favs = quantile(favorite_count, .5),
            n = n(),
            min_date = min(created_at),
            max_date = max(created_at))

glimpse(tweet_summary)

tweet_summary$mean_favs #The $ operator

#inline coding is also very helpful when it comes to writing paper.
#You use `r' 
date_from <- tweet_summary$min_date %>% 
  format("%d %B, %Y")
date_to <- tweet_summary$max_date %>% 
  format("%d %B, %Y")
  
```

There were `r tweet_summary$n` tweets between `r date_from` and `r date_to`.
 
```{r}
verified <- 
  tweets %>% # Start with the original dataset; and then
  group_by(verified) %>% # group it; and then
  summarise(count = n(), # summarise it by those groups
            mean_favs = mean(favorite_count),
            mean_retweets = mean(retweet_count)) %>%
  ungroup()

verified



```
 
 
```{r}
most_fav <- tweets %>%
  group_by(is_quote) %>%
  filter(favorite_count == max(favorite_count)) %>%
  sample_n(size = 1) %>%
  ungroup()



```
 
# Inline coding 2
```{r}

tweets_per_user <- tweets %>%
  count(screen_name, sort = TRUE)

head(tweets_per_user)

unique_users <- nrow(tweets_per_user)
most_prolific <- slice(tweets_per_user, 1) %>% 
  pull(screen_name)
most_prolific_n <- slice(tweets_per_user, 1) %>% 
  pull(n)


```
 There were `r unique_users` unique accounts tweeting about #NationalComingOutDay. `r most_prolific` was the most prolific tweeter, with `r most_prolific_n` tweets.
 
 
# Extra challenge problem

```{r}
ny_data <- readr::read_csv("New_York_City_Leading_Causes_of_Death.csv")


corrected_nydata <-cols(
  Year = col_double(),
  `Leading Cause` = col_character(),
  Sex = col_character(),
  `Race Ethnicity` = col_character(),
  Deaths = col_double(),
  `Death Rate` = col_number(),
  `Age Adjusted Death Rate` = col_number()
)
  
 ny_data <- readr::read_csv("New_York_City_Leading_Causes_of_Death.csv",
                             col_types = corrected_nydata,
                              na = "."
                              )

 summary_nydata <- ny_data %>%
   group_by(`Leading Cause`) %>%
   summarise(mean_death = mean(Deaths, na.rm = TRUE)) %>%
   ggplot(aes(x=`Leading Cause`, y=mean_death)) +
   geom_point(na.rm = TRUE)+
   theme(axis.text.x = element_text(angle = 23, vjust = 1, hjust = 1))+
   scale_y_continuous(name="mean_death", limits = c(0, 1500, breaks= seq(0,1500,100)))


summary_nydata
```

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
